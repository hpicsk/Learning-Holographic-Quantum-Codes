# Default Configuration for Holographic QEC Research
# Full Research Implementation: Hybrid Proposal (P1 + P3)

# =============================================================================
# Dataset Configuration
# =============================================================================
dataset:
  # Total: 5,000 codes
  n_happy: 3500          # HaPPY holographic codes (70%)
  n_ldpc: 1000           # LDPC codes (20%)
  n_random: 500          # Random tensor network codes (10%)

  # HaPPY depth distribution
  happy_depths:
    2: 500               # n ~ 20 qubits
    3: 1000              # n ~ 40 qubits
    4: 1000              # n ~ 80 qubits
    5: 1000              # n ~ 100+ qubits

  # LDPC parameters [[n, k, d]]
  ldpc_params:
    - [7, 4, 3]          # 200 codes (Hamming-like)
    - [15, 7, 5]         # 300 codes (BCH-like)
    - [31, 15, 7]        # 500 codes (Larger LDPC)

  # Data splits
  train_split: 0.70
  val_split: 0.15
  test_split: 0.15

  # Caching
  cache_dir: "data/cache"
  seed: 42

# =============================================================================
# GNN Model Configuration
# =============================================================================
gnn:
  model:
    node_dim: 4            # Input node features (matches dataset: stabilizer.py creates 4)
    hidden_dim: 128        # Hidden dimension
    num_layers: 6          # Number of hypergraph conv blocks
    num_heads: 4           # Attention heads per block
    dropout: 0.1           # Dropout rate
    use_attention: true    # Use multi-head attention

  training:
    epochs: 200
    batch_size: 32
    learning_rate: 0.001
    weight_decay: 0.0001
    scheduler: "cosine"    # Options: cosine, step, plateau
    warmup_epochs: 5
    early_stopping_patience: 20
    gradient_clip: 1.0

  # Curriculum learning
  curriculum:
    enabled: true
    stages:
      - max_qubits: 20
        epochs: 50
      - max_qubits: 50
        epochs: 75
      - max_qubits: 100
        epochs: 75

  # Multi-task learning weights
  loss_weights:
    distance: 1.0
    rate: 0.5
    threshold: 0.3

# =============================================================================
# Hamiltonian Configuration
# =============================================================================
hamiltonians:
  # XXZ model variants
  xxz:
    enabled: true
    n_qubits: [4, 5, 6, 7, 8]
    J_xy: 1.0
    # Anisotropy parameter Delta
    Delta_values: [0.0, 0.5, 1.0, 2.0]
    h_field: 0.0
    periodic: false

  # Transverse Ising model variants
  ising:
    enabled: true
    n_qubits: [4, 5, 6, 7, 8]
    J_coupling: 1.0
    # Transverse field strength
    h_values: [0.0, 0.5, 1.0, 2.0]
    periodic: false

  # Random local Hamiltonians (for variety)
  random_local:
    enabled: true
    n_samples: 500
    n_qubits_range: [4, 8]
    locality: 2

# =============================================================================
# DeepONet Configuration
# =============================================================================
deeponet:
  model:
    code_dim: 128          # From GNN embeddings
    hamiltonian_dim: 8     # Hamiltonian parameters
    trunk_dim: 64          # Time encoding dimension
    hidden_dim: 128        # Hidden layers
    num_basis: 64          # Number of basis functions
    fourier_scale: 10.0    # Fourier feature scale
    dropout: 0.3           # Increased from 0.1 for better regularization
    use_softplus: false    # Disable when using normalized targets

  training:
    epochs: 300
    batch_size: 64
    learning_rate: 0.001   # Increased: physics loss no longer dominates
    weight_decay: 0.001    # Increased from 0.0001 for regularization
    scheduler: "cosine"
    warmup_epochs: 10      # Linear warmup for learning rate
    early_stopping_patience: 30  # Stop if RÂ² doesn't improve

  # Target normalization (Z-score)
  normalization:
    enabled: true
    method: "zscore"

  # Physics-informed constraints (disabled: they create "predict mean" attractor)
  physics:
    enabled: false
    monotonicity_weight: 0.0     # Disabled: constant prediction = zero penalty = local min
    positivity_weight: 0.0
    gradient_regularization: 0.0  # Disabled: smoothness/dt^2 amplifies penalty 10000x
    smoothness_weight: 0.0       # Disabled: prevents learning non-constant functions

  # Time evolution parameters
  dynamics:
    t_max: 20.0
    n_time_steps: 200
    n_trajectories_per_code: 10

# =============================================================================
# Krylov Complexity Configuration
# =============================================================================
krylov:
  max_krylov_dim: 100
  lanczos_tol: 1.0e-10
  t_max: 10.0
  n_time_steps: 100

  # Feature extraction
  features:
    extract_growth_exponent: true
    extract_saturation: true
    extract_lanczos_coeffs: true
    fit_window: [0.05, 0.3]  # Fraction of t_max

# =============================================================================
# Hyperbolic Embedding Configuration
# =============================================================================
embeddings:
  poincare:
    embed_dim: 64
    curvature: -1.0
    max_norm: 0.99

  visualization:
    plot_2d: true
    plot_3d: false
    colorby: "depth"       # Options: depth, distance, family

# =============================================================================
# Analysis Configuration
# =============================================================================
analysis:
  correlation:
    # Statistical tests
    pearson: true
    spearman: true
    mutual_information: true

    # Bootstrap confidence intervals
    bootstrap_samples: 1000
    confidence_level: 0.95

  holographic:
    # AdS/CFT dictionary tests
    entropy_distance: true
    complexity_temperature: true
    page_curve: true
    entanglement_geometry: true

  phase_transition:
    # Parameter scan for phase transitions
    enabled: true
    n_points: 50
    parameters:
      - name: "delta"
        range: [0.0, 3.0]
      - name: "h_field"
        range: [0.0, 2.0]

# =============================================================================
# Output Configuration
# =============================================================================
output:
  base_dir: "results"
  checkpoints_dir: "checkpoints"
  figures_dir: "figures"
  logs_dir: "logs"

  # Logging
  log_level: "INFO"
  tensorboard: true
  wandb:
    enabled: false
    project: "holographic-qec"
    entity: null

  # Checkpointing
  save_every: 10
  keep_best: 3

# =============================================================================
# Hardware Configuration
# =============================================================================
hardware:
  device: "auto"           # auto, cpu, cuda, cuda:0, etc.
  num_workers: 4
  pin_memory: true
  deterministic: false
  seed: 42

# =============================================================================
# Experiment Presets
# =============================================================================
presets:
  # Quick test run
  debug:
    dataset:
      n_happy: 50
      n_ldpc: 20
      n_random: 10
    gnn:
      training:
        epochs: 10
        batch_size: 8
    deeponet:
      training:
        epochs: 20

  # Full research run
  full:
    # Uses all default values above
    pass: true

  # Paper figures only
  figures:
    analysis:
      correlation:
        bootstrap_samples: 5000
    output:
      figures_dir: "paper_figures"
