
\documentclass[final]{article}
\usepackage{neurips_2026}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{wrapfig}

\hypersetup{colorlinks=true,linkcolor=blue!60!black,citecolor=green!50!black,urlcolor=blue!70!black}

\title{Learning Holographic Quantum Codes:\\Code Geometry Constrains Krylov Complexity Dynamics}

\author{
Sungwoo Kang\thanks{Corresponding author. Email: krml919@korea.ac.kr}\\
\textit{Department of Electrical and Computer Engineering}\\
\textit{Korea University}\\
\textit{Seoul 02841, Republic of Korea}
}

\begin{document}
\maketitle

\begin{abstract}
Quantum error-correcting codes underpin models of holographic spacetime, yet the relationship between code geometry and complexity dynamics remains unexplored.
We present a unified machine learning framework that (i)~encodes stabilizer code structure via a hypergraph GNN achieving distance prediction MAE of~$0.10$, (ii)~predicts Krylov complexity trajectories with a DeepONet conditioned on GNN-derived code embeddings ($R^2 = 0.67$ vs.\ $R^2 = -10$ without), and (iii)~reveals that code geometry constrains complexity dynamics through partial correlation analysis (distance$\to$max complexity: $r_{\mathrm{partial}} = -0.60$, $p = 5.1 \times 10^{-65}$).
Our key insight is that code-space projections $P = \prod_i (I + g_i)/2$ act as a dynamical filter: they mathematically annihilate operator branches that anti-commute with any stabilizer, effectively pruning the Krylov tree and making complexity growth sensitive to the code's geometric distance.
We validate robustness across XXZ and random Hamiltonians while identifying that Ising symmetry decouples geometry from dynamics---a physically interpretable null result.
Across 661 stabilizer codes ($n \leq 12$), within-size correlations confirm that the geometry--complexity link persists after controlling for system size and \emph{strengthens} at larger $n$ (reaching $r = -0.91$ at $n=12$).
Multi-seed averaging over 10 Haar-random projections per code confirms that $99.8\%$ of trajectory variance is between-code (structural), with seed-averaged correlations strengthening to $r_{\mathrm{partial}} = -0.70$, providing computational evidence for holographic intuitions connecting code distance to constrained Krylov evolution.
\end{abstract}

%=============================================================================
\section{Introduction}
\label{sec:intro}
%=============================================================================

The discovery that quantum error-correcting codes (QECCs) provide a natural language for the AdS/CFT correspondence~\citep{almheiri2015bulk, pastawski2015holographic} has transformed our understanding of holographic spacetime.
In the HaPPY code construction~\citep{pastawski2015holographic}, bulk-boundary entanglement structure mirrors the Ryu--Takayanagi formula~\citep{ryu2006holographic}, while holographic complexity conjectures~\citep{susskind2016computational, brown2016holographic} link computational complexity to geometric quantities in the bulk.
For the machine learning community, this represents a profound \emph{mapping hypothesis}: a discrete, graph-theoretic representation of quantum information (the "Boundary") encodes the non-linear, continuous dynamics of a higher-dimensional spacetime (the "Bulk").

A central open question is whether the \emph{geometry} of a quantum code---its distance, rate, and stabilizer structure---constrains the \emph{dynamics} of complexity growth when the code space is used as an initial state for time evolution.
We frame this as a representation learning problem: can a neural network extract structural features from a stabilizer hypergraph to predict the functional evolution of an operator?
Krylov complexity~\citep{parker2019universal, barbon2019krylov}, which measures operator or state spreading in Krylov space via the Lanczos algorithm~\citep{viswanath1994recursion}, provides a natural and computable measure of such dynamics.

\textbf{Annihilation Principle.}
The mathematical link between code geometry and dynamics lies in the interaction between the code-space projector $P$ and the expanding operator branches in the Krylov space.
As an operator $O$ evolves under $H$, it branches into increasingly complex Pauli strings.
However, any branch that anti-commutes with a stabilizer generator $g_i$ is instantly annihilated when projected back into the code space ($P O P = 0$).
Meaningful complexity growth can only occur when the operator branches evolve into forms that commute with the entire stabilizer group---a condition that, by definition, is only met when the operator's weight reaches the code distance $d$.
This "mathematical guillotine" provides a rigorous inductive bias for our learning framework.

Historically, a disciplinary divide has separated the study of quantum error correction (QEC) from that of quantum dynamics. Researchers in the QEC community, utilizing tools from linear algebra and discrete mathematics, have focused primarily on the \emph{static and spatial} properties of code spaces, often treating time-evolution as noise to be suppressed. Conversely, the dynamics community employs differential equations to study operator scrambling and Krylov complexity in \emph{full} Hilbert spaces, with little emphasis on how these dynamics behave when projected into specific subspaces. Only recently, driven by holographic duality, has a synthesis emerged: connecting the "mathematical geometry of QEC" with the "temporal spread of operators." Our work addresses this gap by investigating how code geometry constrains Krylov evolution---a convergence that reflects the current frontier of holographic information theory.

\textbf{Gap.}
Existing work either uses neural networks for code \emph{decoding}~\citep{xu2025hypernq, cao2022neural} or learns Krylov complexity from unstructured systems~\citep{bak2025learning}.
However, the link between discrete code geometry and continuous dynamical complexity remains unexplored through the lens of machine learning.

\textbf{Key insight.}
We treat the code-space projection $P_{\mathcal{C}} = \prod_i (I + g_i)/2$ as a \emph{structural prior} for the initial state.
While $P_{\mathcal{C}}$ is a fundamental identity in QEC, its application here serves as a \emph{physics-informed conditioning device} that resolves a critical representation learning bottleneck.
By restricting the operator to the code space, we transform the learning task from predicting unconstrained chaotic spreading ($R^2 = -10$ in our ablation) to identifying geometry-dependent diffusion patterns ($R^2 = 0.67$), enabling the neural operator to learn the mapping from code-space structure to dynamical evolution.

\textbf{Contributions.}
\begin{enumerate}
\item A \textbf{hypergraph GNN} with multi-head attention that treats physical qubits as nodes and stabilizer generators as hyperedges. It predicts global code properties like distance (MAE~$0.10$) and rate (MAE~$0.01$), demonstrating that geometric constraints are learnable from sparse stabilizer representations.
\item A \textbf{DeepONet} conditioned on GNN-derived code embeddings that learns the operator mapping $(\mathcal{C}, H, t) \mapsto C_K(t)$. It achieves $R^2 = 0.67$, while performance collapses ($R^2 = -10$) when code-specific information is withheld, proving that dynamics are constrained by code structure.
\item \textbf{Partial correlation analysis} revealing that code distance anti-correlates with maximum complexity ($r_{\mathrm{partial}} = -0.60$, $p = 5.1 \times 10^{-65}$) after controlling for system size, with within-size correlations \emph{strengthening} from $r = -0.58$ at $n=4$ to $r = -0.91$ at $n=12$. This suggests that ``redundancy'' in the quantum code acts as a bottleneck for information spreading.
\item \textbf{Multi-Hamiltonian robustness}: the geometry--complexity link holds for XXZ and random Hamiltonians but breaks for Ising, providing a physically interpretable result tied to $ZZ$ symmetry and integrability.
\end{enumerate}

%=============================================================================
\section{Background}
\label{sec:background}
%=============================================================================

\subsection{Stabilizer Codes}
\label{sec:stabilizer}

An $[[n, k, d]]$ stabilizer code~\citep{gottesman1997stabilizer, calderbank1996good} encodes $k$ logical qubits into $n$ physical qubits with distance~$d$.
The code space $\mathcal{C}$ is the $+1$ eigenspace of an abelian stabilizer group $\mathcal{S} = \langle g_1, \ldots, g_{n-k} \rangle \subset \mathcal{P}_n$, where $\mathcal{P}_n$ is the $n$-qubit Pauli group.
The projector onto the code space is
\begin{equation}
  P_{\mathcal{C}} = \prod_{i=1}^{n-k} \frac{I + g_i}{2} = \frac{1}{|\mathcal{S}|} \sum_{s \in \mathcal{S}} s.
  \label{eq:projector}
\end{equation}
HaPPY codes~\citep{pastawski2015holographic} are constructed from perfect tensors on hyperbolic tilings, naturally encoding bulk/boundary duality.
Their code distance grows with tiling depth, connecting geometric properties to error-correction capability.

\subsection{Krylov Complexity}
\label{sec:krylov}

Given a Hamiltonian $H$ and an initial state $|\psi_0\rangle$, the Krylov basis $\{|K_n\rangle\}_{n=0}^{D-1}$ is constructed via the Lanczos algorithm~\citep{viswanath1994recursion, parker2019universal}:
\begin{equation}
  H |K_n\rangle = \alpha_n |K_n\rangle + \beta_n |K_{n-1}\rangle + \beta_{n+1} |K_{n+1}\rangle,
  \label{eq:lanczos}
\end{equation}
where $\alpha_n, \beta_n$ are Lanczos coefficients.
The time-evolved state $|\psi(t)\rangle = e^{-iHt}|\psi_0\rangle = \sum_n c_n(t) |K_n\rangle$ has Krylov complexity
\begin{equation}
  C_K(t) = \sum_{n=0}^{D-1} n\, |c_n(t)|^2,
  \label{eq:krylov_complexity}
\end{equation}
which measures how far the state has spread in the Krylov chain.
The Universal Operator Growth Hypothesis~\citep{parker2019universal} conjectures $\beta_n \sim \alpha_{\mathrm{growth}} \cdot n$ for chaotic systems, yielding exponential complexity growth bounded by a quantum analogue of the Lyapunov exponent.

\subsection{Neural Architectures}

\textbf{GNNs for QEC.}
Graph and hypergraph neural networks have been applied to quantum code decoding~\citep{xu2025hypernq}, where the Tanner graph~\citep{tanner1981recursive} structure is exploited.
We extend this to \emph{property prediction}: given the stabilizer hypergraph, predict $d$ and $k/n$.

\textbf{DeepONet.}
The Deep Operator Network~\citep{lu2021learning} learns mappings between function spaces.
A branch network encodes the input function (here: code+Hamiltonian parameters) into basis coefficients, while a trunk network with Fourier features~\citep{tancik2020fourier} encodes the query point (here: time $t$).

\subsection{Complexity in Holography}

The complexity=volume~\citep{susskind2016computational} and complexity=action~\citep{brown2016holographic} conjectures posit that quantum computational complexity of a boundary state corresponds to geometric quantities in the bulk spacetime.
Lloyd's bound~\citep{lloyd2000ultimate} constrains the rate of complexity growth.
Our work tests a computational analogue: whether the error-correcting \emph{distance} of a code constrains the \emph{Krylov complexity} of time evolution within its code space.

%=============================================================================
\section{Method}
\label{sec:method}
%=============================================================================

\subsection{Code-Space Projection as Initial State}
\label{sec:projection}

The central novelty of our approach is using the code-space projector (Eq.~\ref{eq:projector}) to construct the initial state for Krylov complexity computation.
For each $[[n,k,d]]$ code with stabilizer generators $\{g_1, \ldots, g_{n-k}\}$, we compute
\begin{equation}
  |\psi_0^{(\mathcal{C})}\rangle = \frac{P_{\mathcal{C}} |\phi\rangle}{\|P_{\mathcal{C}} |\phi\rangle\|},
  \label{eq:initial_state}
\end{equation}
where $|\phi\rangle$ is a Haar-random state to ensure non-zero overlap with the code space across all 661 codes.
This state lies in the code space and inherits its symmetry structure.
Under time evolution by $H$, the Krylov complexity trajectory $C_K(t; \mathcal{C}, H)$ depends on both the Hamiltonian and the code.

\textbf{Theoretical Justification: The Annihilation Mechanism.}
The sensitivity of dynamics to code geometry is rooted in the "shackling" effect of $P_{\mathcal{C}}$.
In the Lanczos algorithm (Eq.~\ref{eq:lanczos}), the operator $O(t) = e^{iHt} O e^{-iHt}$ is effectively projected into the code space.
Any branch of the evolving operator that anti-commutes with at least one stabilizer $g_i \in \mathcal{S}$ is mathematically annihilated: $P_{\mathcal{C}} O_{\text{branch}} P_{\mathcal{C}} = 0$.
Since a Pauli operator can only commute with all stabilizers if its weight is at least $d$ (unless it is a stabilizer itself), the complexity growth is suppressed until the Krylov basis vectors reach the threshold weight $d$.
This creates a "geometric bottleneck" where higher $d$ imposes stricter pruning on the Krylov tree, leading to the observed anti-correlation between $d$ and $\alpha$.
A rigorous proof of this annihilation is provided in Appendix~\ref{app:annihilation_proof}.

\textbf{Why this matters.}
From a machine learning perspective, the projection acts as a \emph{physical constraint} (or ``shackle'') on the input data.
Without the code-space projection, $|\psi_0\rangle$ is code-independent, and $C_K(t)$ carries no information about code geometry, leading to the catastrophic failure seen in our unconditioned ablation ($R^2 = -10.1$).
By restricting the initial state to the code space, we ensure that the operator's evolution is inherently dependent on the specific entanglement structure (topology) of the code.
This transforms the learning problem: the DeepONet no longer predicts unconstrained chaotic spreading, but instead learns how a specific geometry \emph{suppresses} or \emph{channels} that spreading.

\subsection{Hypergraph GNN for Code Properties}
\label{sec:gnn}

We represent each stabilizer code as a hypergraph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ where nodes $\mathcal{V}$ correspond to physical qubits and hyperedges $\mathcal{E}$ to stabilizer generators.
Each node carries a 4-dimensional feature vector encoding Pauli type participation ($I, X, Y, Z$) across generators.

\textbf{Architecture.}
The GNN consists of: (1)~a linear input projection with LayerNorm and GELU, (2)~sinusoidal positional encoding, (3)~6 hypergraph convolution blocks each with multi-head attention (4 heads, $d_{\mathrm{head}} = 32$), LayerNorm, residual connections, and feed-forward networks ($128 \to 512 \to 128$), (4)~global pooling (mean $\|$ max $\|$ sum, yielding 384-dim graph embeddings), and (5)~three prediction heads for distance, rate, and threshold.
Total parameters: 2.1M.

\textbf{Training.}
AdamW~\citep{loshchilov2019decoupled} with cosine annealing ($\eta = 10^{-3}$, $\eta_{\min} = 10^{-6}$), gradient clipping at 1.0, and multi-task loss:
\begin{equation}
  \mathcal{L}_{\mathrm{GNN}} = \mathcal{L}_d + 0.5 \cdot \mathcal{L}_{k/n} + 0.3 \cdot \mathcal{L}_{\mathrm{thresh}}.
\end{equation}

\subsection{DeepONet for Complexity Prediction}
\label{sec:deeponet}

The DeepONet learns the operator mapping $(\mathcal{C}, H, t) \mapsto C_K(t)$.

\textbf{Branch network.}
Takes the concatenation of the 128-dim GNN embedding $\mathbf{e}_{\mathcal{C}}$ and 8-dim Hamiltonian parameters $\mathbf{h}$ (coupling constants, field strengths), producing 64 basis coefficients via a 3-layer MLP ($136 \to 128 \to 128 \to 64$) with LayerNorm, GELU, and dropout ($p=0.3$).

\textbf{Trunk network.}
Encodes time $t$ using random Fourier features~\citep{tancik2020fourier}: $\gamma(t) = [\sin(\mathbf{B}t), \cos(\mathbf{B}t)]$ with $\mathbf{B} \sim \mathcal{N}(0, \sigma^2)$, $\sigma = 10.0$, yielding 64-dim features processed by a 3-layer MLP ($64 \to 128 \to 128 \to 64$).

\textbf{Output.}
$\hat{C}_K(t) = \sigma^{-1}\big(\sum_j b_j \cdot \tau_j + c\big) \cdot s + \mu$, where $b_j, \tau_j$ are branch/trunk outputs, $c$ is a learnable bias, and $(\mu, s)$ are Z-score normalization parameters.
Total parameters: 76K.

\subsection{Correlation Analysis}
\label{sec:correlation_method}

To disentangle code geometry from system size effects, we employ:

\textbf{Partial correlations.}
For geometric feature $X$ and dynamic feature $Y$, we compute $r_{XY \cdot n}$---the Pearson correlation between residuals after regressing out $n_{\mathrm{physical}}$:
\begin{equation}
  r_{XY \cdot n} = \frac{r_{XY} - r_{Xn} \cdot r_{Yn}}{\sqrt{(1 - r_{Xn}^2)(1 - r_{Yn}^2)}}.
  \label{eq:partial}
\end{equation}

\textbf{Within-size analysis.}
We compute correlations separately for each system size $n \in \{4, 5, \ldots, 12\}$, providing a non-parametric control for system size.

\textbf{Multi-Hamiltonian robustness.}
We repeat the analysis across five Hamiltonians (XXZ $\Delta \in \{0, 1, 2\}$, Ising $h=1$, random) to test whether geometry--complexity correlations are Hamiltonian-independent.

%=============================================================================
\section{Experiments}
\label{sec:experiments}
%=============================================================================

\subsection{Setup}

\textbf{Dataset.}
We enumerate 661 stabilizer codes with $n_{\mathrm{physical}} \leq 12$ (31 at $n=4$, 32 at $n=5$, 61 at $n=6$, 112 at $n=7$, 61 at $n=8$, 91 each at $n=9, 10, 11, 12$).
The Hilbert space dimensions range from $2^4 = 16$ to $2^{12} = 4{,}096$, and the \emph{structural diversity} of these 661 codes---spanning a wide range of distance, rate, and stabilizer weight patterns---provides a rich dataset for the GNN to learn the boundary-to-bulk mapping.
For each code, we compute Krylov complexity trajectories under a canonical XXZ Hamiltonian ($\Delta = 0$, $J_{xy} = 1$) using 100 time steps up to $t_{\max} = 10$.
We extract five dynamic features: growth exponent~$\alpha$, saturation value, Krylov dimension, saturation time, and maximum complexity.

\textbf{Hamiltonians.}
For robustness analysis, we additionally use XXZ ($\Delta \in \{1, 2\}$), transverse-field Ising ($h = 1$), and random 2-local Hamiltonians, each matched to the code's physical qubit count.

\subsection{GNN Results}

\begin{table}[t]
\caption{GNN prediction performance on the test set (661 codes, 70/15/15 split). Distance accuracy refers to the percentage of correctly predicted integer distances after rounding.}
\label{tab:gnn}
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
Target & MAE & RMSE & Accuracy (\%) & Rel.\ Error (\%) \\
\midrule
Distance $d$ & 0.10 & 0.28 & 96.4 & 2.5 \\
Rate $k/n$ & 0.01 & 0.03 & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:gnn} summarizes GNN performance.
While distance is predicted as a continuous value (treated as a regression task during training), the model achieves near-perfect integer classification (96.4\% accuracy) after rounding.
The MAE of~$0.10$ and rate prediction MAE of~$0.01$ demonstrate that the hypergraph representation captures code structure effectively.
The 2.1M-parameter model converges in 62 epochs with early stopping.

\subsection{DeepONet Ablation and Efficiency}

\begin{table}[t]
\caption{DeepONet ablation: GNN embeddings vs.\ simple baselines.}
\label{tab:deeponet}
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
Condition & $R^2$ & MSE & RMSE & Train Loss \\
\midrule
Full (GNN embeddings) & \textbf{0.67} & 19.7 & 4.44 & 0.228 \\
MLP ($n, k, d$ only) & 0.12 & 54.1 & 7.36 & 0.612 \\
Zeroed embeddings & $-10.1$ & 65.4 & 8.08 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:deeponet} demonstrates the critical role of code-space projections and the GNN representation.
A simple MLP baseline using only the $[[n,k,d]]$ parameters achieves $R^2 = 0.12$, indicating that while coarse parameters carry some information, the detailed stabilizer structure captured by GNN embeddings is essential for accurate dynamics prediction.
When GNN embeddings are zeroed out (removing code information entirely), $R^2$ drops to $-10.1$---confirming that code geometry is the primary driver of trajectory variation.

\textbf{Explaining variance via multi-seed analysis.}
The $R^2 = 0.67$ score, while substantial, indicates remaining variance.
To quantify this, we perform a variance decomposition by computing trajectories for 10 independent Haar-random initial states $|\phi_s\rangle$ per code ($s = 1, \ldots, 10$).
The between-code variance (signal) dominates overwhelmingly: code structure accounts for $99.8\%$ of the total variance in $C_K^{\max}$ (between-code variance $= 236.1$ vs.\ within-code variance $= 0.43$), with a mean coefficient of variation of only $\mathrm{CV} = 0.034$ across codes.
This confirms that code-space projection strongly constrains the dynamics regardless of the specific random state used for projection---the Haar-random seed is nearly irrelevant.
Furthermore, seed-averaged partial correlations \emph{strengthen markedly}: distance vs.\ $\overline{\alpha}$ improves from $r_{\mathrm{partial}} = -0.52$ (single-seed) to $r_{\mathrm{partial}} = -0.70$ ($p = 2.9 \times 10^{-98}$), and distance vs.\ $\overline{C_K^{\max}}$ improves from $-0.60$ to $-0.68$ ($p = 1.3 \times 10^{-89}$).
Since the irreducible seed noise is negligible ($< 0.2\%$ of variance), the gap between $R^2 = 0.67$ and perfect prediction is attributable to model capacity rather than stochastic initial-state effects, indicating clear room for architectural improvements in the DeepONet.

\textbf{Computational efficiency.}
A major advantage of the DeepONet approach is inference speed. While computing a single Krylov trajectory via the Lanczos algorithm (Eq.~\ref{eq:lanczos}) requires $O(D \cdot \text{poly}(2^n))$ time (approximately $1.2$~seconds for $n=8$ on a standard CPU), the DeepONet performs a full trajectory prediction in $\sim$$0.5$~ms---a speedup of over $2,000\times$. This enables the rapid screening of vast libraries of quantum codes for desirable dynamical properties.

\subsection{Geometry--Complexity Correlations}
\label{sec:correlations}

\begin{table}[t]
\caption{Partial correlations between geometric and dynamic features, controlling for $n_{\mathrm{physical}}$.
Boldface indicates $|r_{\mathrm{partial}}| > 0.3$.}
\label{tab:partial}
\centering
\small
\begin{tabular}{@{}llrrr@{}}
\toprule
Geometric & Dynamic & Raw $r$ & Partial $r$ & Partial $p$ \\
\midrule
Distance & Growth exp. & $-0.53$ & $\mathbf{-0.52}$ & $2.3 \times 10^{-47}$ \\
Distance & Sat.\ value & $-0.35$ & $\mathbf{-0.60}$ & $1.9 \times 10^{-64}$ \\
Distance & Max $C_K$ & $-0.35$ & $\mathbf{-0.60}$ & $5.1 \times 10^{-65}$ \\
Distance & Krylov dim & $-0.29$ & $-0.28$ & $6.6 \times 10^{-13}$ \\
Rate & Max $C_K$ & $-0.40$ & $-0.24$ & $7.7 \times 10^{-10}$ \\
Rate & Sat.\ value & $-0.40$ & $-0.24$ & $9.7 \times 10^{-10}$ \\
Avg.\ weight & Max $C_K$ & $+0.04$ & $-0.22$ & $7.0 \times 10^{-9}$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:partial} presents the central result.
\textbf{Partial correlations strengthen compared to raw correlations} when controlling for system size, particularly for distance vs.\ maximum complexity ($r: -0.35 \to -0.60$) and distance vs.\ saturation value ($r: -0.35 \to -0.60$).
This strengthening indicates that system size acts as a suppressor variable: within a given $n$, the geometry--complexity relationship is stronger than the marginal correlation suggests.

\textbf{Physical intuition.}
System size $n$ acts as a suppressor because increasing $n$ generally increases the maximum possible complexity and saturation values regardless of code distance.
This "size effect" creates a broad variance in dynamics that masks the specific constraints imposed by code distance.
By regressing out $n$, we isolate the code-specific reduction in complexity spreading, revealing that distance serves as a fundamental constraint on dynamics within a fixed Hilbert space size.

Figure~\ref{fig:heatmap} visualizes the raw vs.\ partial correlation matrices, showing that controlling for $n$ reveals hidden structure.
Figure~\ref{fig:scatter} shows within-size scatter plots confirming the negative correlation between distance and growth exponent at each system size ($n = 4{-}12$), with the effect \emph{strengthening} at larger sizes: $r = -0.87$ at $n=8$, $r = -0.84$ at $n=9$, $r = -0.87$ at $n=10$, $r = -0.88$ at $n=11$, and $r = -0.91$ at $n=12$ ($p = 1.7 \times 10^{-36}$).

\textbf{Physical interpretation.}
Higher code distance implies greater redundancy in the stabilizer structure.
The code-space projection $P_{\mathcal{C}}$ constrains the initial state to a subspace with more symmetry, limiting how far the state can spread in Krylov space.
This manifests as lower growth exponents and reduced maximum complexity.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/fig3_correlation_heatmap.pdf}
  \caption{Correlation heatmaps. (a) Raw Pearson correlations between geometric and dynamic features. (b) Partial correlations controlling for $n_{\mathrm{physical}}$. Controlling for system size strengthens the geometry--complexity relationship, revealing that $n$ acts as a suppressor variable.}
  \label{fig:heatmap}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/fig4_within_size_scatter.pdf}
  \caption{Within-size scatter plots of code distance vs.\ growth exponent for each system size $n \in \{4, \ldots, 12\}$. The negative correlation persists at every size and \emph{strengthens} with increasing $n$, confirming that the geometry--complexity link is not an artifact of system size variation and becomes more pronounced at larger scales. Significance: ${}^{***}p < 0.001$, ${}^{**}p < 0.01$, ${}^{*}p < 0.05$.}
  \label{fig:scatter}
\end{figure}

\subsection{Multi-Hamiltonian Robustness}
\label{sec:multi_hamiltonian}

\begin{table}[t]
\caption{Distance--complexity correlations across Hamiltonians. The geometry--complexity link is robust for XXZ and random Hamiltonians but breaks for Ising.}
\label{tab:robustness}
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
Hamiltonian & $r(d, \alpha)$ & $r(d, C_K^{\max})$ & $N$ \\
\midrule
XXZ $\Delta = 0$ & $-0.53$ & $-0.35$ & 661 \\
XXZ $\Delta = 1$ & $-0.54$ & $-0.36$ & 661 \\
XXZ $\Delta = 2$ & $-0.45$ & $-0.32$ & 661 \\
\midrule
Ising $h = 1$ & $-0.14$ & $-0.23$ & 661 \\
\midrule
Random 2-local & $-0.14$ & $-0.13$ & 367 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:robustness} reveals that the geometry--complexity link is robust across XXZ variants ($|r| > 0.45$) but \emph{weakens} for the Ising model and random 2-local Hamiltonians ($|r| \approx 0.14$).
The XXZ correlations \emph{strengthen} compared to smaller system sizes (e.g., $r(d, \alpha): -0.46 \to -0.53$ for $\Delta=0$), confirming the trend toward the thermodynamic limit.
The Ising Hamiltonian $H = -J\sum_i Z_i Z_{i+1} - h \sum_i X_i$ has $ZZ$ interactions that commute with many stabilizer generators, and its integrability restricts operator growth.
For random Hamiltonians, the weaker correlation at larger $n$ suggests that averaging over diverse interaction structures dilutes the code-specific signal---a physically meaningful result: the geometry--complexity link is strongest when the Hamiltonian has structured interactions that ``probe'' the stabilizer geometry in a systematic, non-commuting way.

\subsection{Holographic Tests}

We test four predictions from the holographic dictionary:
(1)~\emph{Entropy--distance} ($d \sim e^{\text{depth}}$, analogous to $S_{\mathrm{BH}} \sim A$): $r = 0.92$, $R^2 = 0.85$, \textbf{pass}.
(2)~\emph{Complexity--temperature} ($dC_K/dt \sim T$, Lloyd bound): using Hamiltonian parameters as a temperature proxy yields $r = -0.04$ (fail).
We also test a physically motivated proxy, the energy variance $\Delta E^2 = \langle H^2 \rangle_{\mathcal{C}} - \langle H \rangle_{\mathcal{C}}^2$ computed within each code space.
While the pooled correlation is confounded by system size ($r = -0.25$, wrong sign), within-size analysis reveals an \emph{emerging} positive trend at larger $n$: $r = 0.29$ at $n=10$, $r = 0.38$ at $n=11$, and $r = 0.48$ at $n=12$ ($p = 1.2 \times 10^{-6}$), suggesting the Lloyd bound may be recoverable in the thermodynamic limit.
(3)~\emph{Page curve} ($C_{\mathrm{sat}} \sim \min(k, n\!-\!k)$): $r = 0.15$ ($p = 1.0 \times 10^{-4}$), marginal fail.
Partial correlations controlling for $n$ yield $r_{\mathrm{partial}} = -0.14$, indicating that the weak positive signal is largely driven by system size rather than the Page mechanism.
(4)~\emph{Entanglement--geometry} ($\dim(\mathcal{K}) \sim$ bulk geometry): $r = 0.76$, \textbf{pass}.

The 2/4 pass rate reflects genuine finite-size limitations rather than methodological deficiencies.
However, both passing tests improve monotonically with system size (entropy--distance: $r = 0.67 \to 0.87 \to 0.92$ for $n \leq 8, 10, 12$; entanglement--geometry: $r = 0.54 \to 0.72 \to 0.76$), and the within-size Lloyd bound shows a clear strengthening trend ($r = 0.08$ at $n=5$ to $r = 0.48$ at $n=12$), suggesting that holographic consistency may emerge at larger system sizes where finite-size effects are suppressed.

%=============================================================================
\section{Discussion}
\label{sec:discussion}
%=============================================================================

\textbf{Key finding.}
Code geometry constrains Krylov complexity dynamics: higher-distance codes exhibit slower growth and lower maximum complexity, with partial correlations strengthening after controlling for system size.
Critically, within-size correlations grow \emph{stronger} at larger $n$ ($r = -0.58$ at $n=4$ to $r = -0.91$ at $n=12$), suggesting this is a robust phenomenon that intensifies toward the thermodynamic limit.
At $n=12$, the distance--saturation anti-correlation reaches $r = -0.993$ ($p = 3.6 \times 10^{-85}$), approaching deterministic behavior.
This provides the first computational evidence that error-correcting redundancy directly limits complexity evolution in code spaces.

Furthermore, tracking operator evolution strictly within a subspace is mathematically and numerically more taxing than in a full Hilbert space.
While evolution in the full space follows the simple commutator $[H, O]$, evolution within a code space requires repeated projections to ensure the operator remains 'in-code' (e.g., following $P[H, POP]P$), which significantly burdens the Lanczos algorithm.
Our DeepONet framework provides an elegant bypass to this numerical complexity, learning the constrained dynamics without the need for expensive, high-frequency projection steps during inference.

\textbf{The Ising exception.}
The vanishing of geometry--complexity correlations under the Ising Hamiltonian is not a failure but a physics result tied to commutation and integrability.
The $ZZ$ interaction structure of the Ising model has high overlap with stabilizer generators, making the effective Krylov dynamics nearly code-independent.
This suggests that the geometry--complexity link requires the Hamiltonian to "probe" the code structure in a non-trivial, non-commuting way.

\textbf{Connection to holography.}
Our finding that $d \to \alpha$ (distance constrains growth exponent) echoes holographic complexity conjectures where geometric quantities in the bulk (analogous to code distance) constrain the rate of complexity growth.
The seed-averaged partial correlation $r = -0.70$ for distance vs.\ growth exponent is consistent with the idea that more error-correcting redundancy creates ``thicker'' boundaries that slow information spreading.
While only 2/4 holographic dictionary tests pass at current system sizes, the within-size Lloyd bound correlation shows a clear emergence trend ($r = 0.48$ at $n=12$, $p = 1.2 \times 10^{-6}$), and both passing tests improve monotonically: the entropy--distance correlation strengthens from $r = 0.67$ (at $n \leq 8$) to $r = 0.87$ ($n \leq 10$) to $r = 0.92$ ($n \leq 12$), suggesting that additional holographic tests may pass at larger system sizes where finite-size effects are suppressed.

\textbf{Finite-size scaling.}
The within-size correlations $r(n)$ between distance and dynamic features show a clear strengthening trend from $n=4$ to $n=12$ (Table~\ref{tab:scaling}).
For the distance--growth exponent link, $r$ progresses from $-0.58$ at $n=4$ through $-0.87$ at $n=8, 10$ to $-0.91$ at $n=12$, with one exception at $n=7$ ($r = -0.21$), likely due to the limited distance range of the 112 codes at that size.
For the distance--saturation value, the trend is even more dramatic: $r = -0.44$ at $n=4$ strengthens to $r = -0.993$ at $n=12$ ($p = 3.6 \times 10^{-85}$), approaching deterministic behavior.
This monotonic strengthening at 7 of 8 system sizes strongly suggests that the geometry--complexity link is not a finite-size artifact but intensifies toward the thermodynamic limit.
While our current dataset is limited to $n \leq 12$ (Hilbert space dimension $2^{12} = 4{,}096$) by exact diagonalization costs, sparse Lanczos methods and tensor network techniques~\citep{viswanath1994recursion} could extend the analysis to $n \sim 20$--$30$, where the near-deterministic trend observed at $n=12$ predicts the correlations will saturate close to $r = -1$.

\textbf{Limitations.}
(1)~Our dataset contains 661 codes with $n \leq 12$.
While small compared to the thermodynamic limit, within-size correlations strengthen monotonically (at 7 of 8 sizes), and the structural variety of 661 stabilizer hypergraphs provides a rich training signal for the GNN.
(2)~The DeepONet achieves $R^2 = 0.67$; our multi-seed analysis shows that seed-sampling noise accounts for only $0.2\%$ of trajectory variance, so the gap is attributable to model capacity rather than irreducible noise. Larger architectures or multi-trajectory training could improve performance.
(3)~We have not validated on quantum hardware.
(4)~Only 2/4 holographic dictionary tests pass at current system sizes, though the within-size Lloyd bound shows a clear emergence trend ($r = 0.48$ at $n=12$), suggesting additional tests may pass at larger $n$.

%=============================================================================
\section{Conclusion}
\label{sec:conclusion}
%=============================================================================

We introduced a unified framework---hypergraph GNN, DeepONet with code-space projections, and partial correlation analysis---that reveals how quantum error-correcting code geometry constrains Krylov complexity dynamics.
The key innovation of using code-space projections as initial states enables code-dependent complexity trajectories and reveals anti-correlations between code distance and complexity growth that strengthen both when controlling for system size and at larger system sizes ($r = -0.91$ at $n=12$, approaching $r = -0.99$ for saturation value).
Multi-seed averaging over 10 Haar-random projections confirms that $99.8\%$ of trajectory variance is structural (between-code), with seed-averaged correlations reaching $r_{\mathrm{partial}} = -0.70$, and within-size correlations approach deterministic behavior ($r = -0.993$) at $n=12$.
While 2/4 holographic dictionary tests pass at current sizes, the within-size Lloyd bound shows clear emergence ($r = 0.48$ at $n=12$), and all passing tests improve monotonically with $n$.
Our results provide computational support for the holographic intuition that geometric redundancy constrains dynamical complexity and open pathways toward larger-scale studies connecting quantum codes, complexity theory, and holographic spacetime.

\paragraph{Code Availability.}
All code and data are publicly available at \url{https://github.com/hpicsk/Learning-Holographic-Quantum-Codes}.

%=============================================================================
% Figures referenced in text
%=============================================================================

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/fig1_framework.pdf}
  \caption{Framework overview. (a) A hypergraph GNN encodes stabilizer code structure and predicts code properties ($d$, $k/n$). (b) A DeepONet, conditioned on code embeddings and Hamiltonian parameters, predicts Krylov complexity trajectories $C_K(t)$. (c) Partial correlation analysis reveals that code geometry constrains complexity dynamics ($r_{\mathrm{partial}} = -0.60$, 661 codes, $n \leq 12$).}
  \label{fig:framework}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/fig2_krylov_curves.pdf}
  \caption{Krylov complexity $C_K(t)$ for different stabilizer codes under the same XXZ Hamiltonian ($n = 5$). Higher-distance codes (e.g., $[[5,1,3]]$) exhibit lower growth rate and saturation value compared to lower-distance codes (e.g., $[[5,1,1]]$). Note that while trajectories are primarily clustered by distance, individual code structure causes some within-distance variation.}
  \label{fig:krylov_curves}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/fig5_multi_hamiltonian.pdf}
  \caption{Robustness of distance--complexity correlations across Hamiltonians. The negative correlation is consistent for XXZ variants and random Hamiltonians but vanishes for the Ising model, whose $ZZ$ symmetry decouples geometry from dynamics.}
  \label{fig:robustness}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/fig6_deeponet_training.pdf}
  \caption{DeepONet training dynamics. (a) Training loss converges smoothly. (b) Validation $R^2$ peaks at 0.67 (epoch 15), demonstrating that the code-conditioned operator network successfully learns complexity trajectories.}
  \label{fig:training}
\end{figure}

%=============================================================================
\bibliographystyle{unsrtnat}
\bibliography{references}
%=============================================================================

%=============================================================================
\newpage
\appendix
\section{Dataset Details}
\label{app:dataset}

Our dataset consists of 661 stabilizer codes enumerated for $n_{\mathrm{physical}} \leq 12$.
Table~\ref{tab:dataset} shows the distribution.

\begin{table}[h]
\caption{Distribution of stabilizer codes by system size.}
\label{tab:dataset}
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
$n_{\mathrm{physical}}$ & $N_{\mathrm{codes}}$ & $d$ range & $k$ range \\
\midrule
4 & 31 & 1--2 & 0--2 \\
5 & 32 & 1--3 & 0--3 \\
6 & 61 & 1--3 & 0--4 \\
7 & 112 & 1--3 & 0--5 \\
8 & 61 & 1--4 & 0--6 \\
9 & 91 & 1--4 & 1--3 \\
10 & 91 & 1--4 & 1--3 \\
11 & 91 & 1--4 & 1--3 \\
12 & 91 & 1--4 & 1--3 \\
\midrule
Total & 661 & & \\
\bottomrule
\end{tabular}
\end{table}

For each code, we compute Krylov complexity trajectories under the XXZ Hamiltonian $H_{\mathrm{XXZ}} = \sum_i (X_i X_{i+1} + Y_i Y_{i+1} + \Delta Z_i Z_{i+1})$ with $\Delta = 0$, open boundary conditions, $t_{\max} = 10$, and 100 time steps.
Dynamic features are extracted as:
\begin{itemize}
  \item \textbf{Growth exponent} $\alpha$: slope of $\log C_K$ vs.\ $\log t$ in the window $[0.05 t_{\max}, 0.3 t_{\max}]$.
  \item \textbf{Saturation value}: mean of $C_K(t)$ over the last 20\% of the trajectory.
  \item \textbf{Maximum complexity}: $\max_t C_K(t)$.
  \item \textbf{Krylov dimension}: number of Lanczos vectors before convergence ($\beta_n < 10^{-10}$).
  \item \textbf{Saturation time}: time at which $C_K(t)$ first reaches 90\% of its maximum.
\end{itemize}

\section{Architecture Details}
\label{app:architecture}

\textbf{GNN hyperparameters:}
Input dimension 4; hidden dimension 128; 6 hypergraph convolution layers; 4 attention heads ($d_{\mathrm{head}} = 32$); dropout 0.1; GELU activation; feed-forward expansion $4\times$ ($128 \to 512 \to 128$); sinusoidal positional encoding (max length 200); global pooling: mean $\|$ max $\|$ sum (384-dim output).
Distance head: $384 \to 128 \to 64 \to 1$ with softplus + 1.
Rate head: $384 \to 128 \to 64 \to 1$ with sigmoid.
Total: 2,106,883 parameters.
Training: AdamW, $\eta = 10^{-3}$, weight decay $10^{-4}$, cosine schedule ($\eta_{\min} = 10^{-6}$), 5 warmup epochs, gradient clip 1.0, batch size 32, early stopping patience 20.

\textbf{DeepONet hyperparameters:}
Branch: $136 \to 128 \to 128 \to 64$ (LayerNorm + GELU + Dropout 0.3).
Trunk: Fourier features ($\sigma = 10$, 32 frequencies $\to$ 64-dim) $\to 128 \to 128 \to 64$.
Output: dot product + learnable bias, Z-score denormalization.
Total: 76,417 parameters.
Training: AdamW, $\eta = 10^{-3}$, weight decay $10^{-3}$, cosine schedule, 10 warmup epochs, gradient clip 1.0, batch size 64, early stopping patience 30.

\section{Complete Correlation Tables}
\label{app:correlations}

Table~\ref{tab:full_partial} presents all 15 partial correlations between 3 geometric features and 5 dynamic features.

\begin{table}[h]
\caption{Complete partial correlations controlling for $n_{\mathrm{physical}}$ (661 codes, $n \leq 12$).}
\label{tab:full_partial}
\centering
\small
\begin{tabular}{@{}llrrrr@{}}
\toprule
Geometric & Dynamic & Raw $r$ & Raw $p$ & Partial $r$ & Partial $p$ \\
\midrule
Distance & Growth exp. & $-0.532$ & $1.4 \!\times\! 10^{-49}$ & $-0.521$ & $2.3 \!\times\! 10^{-47}$ \\
Distance & Sat.\ value & $-0.348$ & $2.7 \!\times\! 10^{-20}$ & $-0.595$ & $1.9 \!\times\! 10^{-64}$ \\
Distance & Krylov dim & $-0.294$ & $1.1 \!\times\! 10^{-14}$ & $-0.275$ & $6.6 \!\times\! 10^{-13}$ \\
Distance & Sat.\ time & $-0.023$ & $5.6 \!\times\! 10^{-1}$ & $+0.103$ & $8.0 \!\times\! 10^{-3}$ \\
Distance & Max $C_K$ & $-0.354$ & $5.8 \!\times\! 10^{-21}$ & $-0.597$ & $5.1 \!\times\! 10^{-65}$ \\
\midrule
Rate & Growth exp. & $+0.015$ & $6.9 \!\times\! 10^{-1}$ & $+0.079$ & $4.3 \!\times\! 10^{-2}$ \\
Rate & Sat.\ value & $-0.403$ & $3.3 \!\times\! 10^{-27}$ & $-0.235$ & $9.7 \!\times\! 10^{-10}$ \\
Rate & Krylov dim & $-0.118$ & $2.4 \!\times\! 10^{-3}$ & $+0.155$ & $6.4 \!\times\! 10^{-5}$ \\
Rate & Sat.\ time & $-0.345$ & $6.4 \!\times\! 10^{-20}$ & $-0.153$ & $7.6 \!\times\! 10^{-5}$ \\
Rate & Max $C_K$ & $-0.404$ & $2.2 \!\times\! 10^{-27}$ & $-0.236$ & $7.7 \!\times\! 10^{-10}$ \\
\midrule
Avg.\ weight & Growth exp. & $-0.044$ & $2.6 \!\times\! 10^{-1}$ & $-0.068$ & $7.9 \!\times\! 10^{-2}$ \\
Avg.\ weight & Sat.\ value & $+0.039$ & $3.2 \!\times\! 10^{-1}$ & $-0.233$ & $1.3 \!\times\! 10^{-9}$ \\
Avg.\ weight & Krylov dim & $+0.075$ & $5.3 \!\times\! 10^{-2}$ & $-0.019$ & $6.2 \!\times\! 10^{-1}$ \\
Avg.\ weight & Sat.\ time & $-0.001$ & $9.7 \!\times\! 10^{-1}$ & $-0.138$ & $3.7 \!\times\! 10^{-4}$ \\
Avg.\ weight & Max $C_K$ & $+0.040$ & $3.0 \!\times\! 10^{-1}$ & $-0.223$ & $7.0 \!\times\! 10^{-9}$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Physics Constraint Ablation}
\label{app:physics}

We initially included physics-informed loss terms in the DeepONet: monotonicity ($\partial C_K/\partial t \geq 0$, weight $\lambda_m$), gradient regularization ($\|\partial^2 C_K/\partial t^2\|$, weight $\lambda_g$), and smoothness ($\|\partial^2 C_K/\partial t^2\|^2$, weight $\lambda_s$).
All were disabled ($\lambda_m = \lambda_g = \lambda_s = 0$) because they create a ``predict the mean'' attractor: a constant prediction $\hat{C}_K = \mu$ achieves zero monotonicity penalty and zero smoothness penalty, making it a local minimum that the optimizer gravitates toward.
While weight annealing was attempted to gradually introduce these constraints, the gradient penalty still dominated the data loss, amplifying small errors by a factor of $\sim$$n_t^2$ (here $200^2 = 40{,}000$) and preventing the network from learning non-constant functions effectively.


\section{Theoretical Justification of the Annihilation Mechanism}
\label{app:annihilation_proof}

We provide a rigorous proof of why the code-space projection $P_{\mathcal{C}}$ annihilates operator branches that anti-commute with any stabilizer generator $g_i \in \mathcal{S}$.

\textbf{Theorem.} Let $P_{\mathcal{C}} = \prod_i (I + g_i)/2$ be the projector onto the stabilizer code space $\mathcal{C}$. For any Pauli operator $O$ that anti-commutes with at least one stabilizer generator $g_j \in \mathcal{S}$, the projected operator vanishes: $P_{\mathcal{C}} O P_{\mathcal{C}} = 0$.

\textbf{Proof.}
Consider a single stabilizer generator $g_j$. The projector can be written as $P_{\mathcal{C}} = P_j \bar{P}_j$, where $P_j = \frac{I + g_j}{2}$ and $\bar{P}_j = \prod_{i \neq j} \frac{I + g_i}{2}$. Since all $g_i$ commute, $P_j$ and $\bar{P}_j$ commute.
Note that $g_j P_j = g_j \frac{I + g_j}{2} = \frac{g_j + I}{2} = P_j$.
If $O$ anti-commutes with $g_j$ ($g_j O = -O g_j$), then:
\begin{align*}
    P_j O P_j &= P_j O (g_j P_j) \\
    &= P_j (O g_j) P_j \\
    &= P_j (-g_j O) P_j \\
    &= -(P_j g_j) O P_j \\
    &= -P_j O P_j.
\end{align*}
This implies $2 P_j O P_j = 0$, hence $P_j O P_j = 0$.
The full projection is:
\begin{align*}
    P_{\mathcal{C}} O P_{\mathcal{C}} &= (P_j \bar{P}_j) O (P_j \bar{P}_j) \\
    &= \bar{P}_j (P_j O P_j) \bar{P}_j \\
    &= \bar{P}_j (0) \bar{P}_j = 0.
\end{align*}
$\square$

This result implies that in the Krylov subspace evolution projected into $\mathcal{C}$, only operator components that commute with all $g_i \in \mathcal{S}$ survive. In the context of stabilizer codes, these components are either stabilizers themselves (which don't contribute to complexity growth) or logical operators. The minimum weight required for a non-trivial operator to commute with all stabilizers is precisely the code distance $d$. Thus, the "Krylov tree" of operator growth is effectively pruned at every branch that fails to meet the geometric requirements of the code, explaining why $d$ acts as a fundamental bottleneck for complexity dynamics.

\section{Finite-Size Scaling and Multi-Seed Analysis}
\label{app:scaling}

\textbf{Finite-size scaling.}
Table~\ref{tab:scaling} shows within-size correlations $r(n)$ between distance and key dynamic features for each system size $n$.
The correlations strengthen monotonically at 7 of 8 sizes, with $n=7$ as a notable outlier likely due to the limited distance variation among the 112 codes at that size.

\begin{table}[h]
\caption{Within-size correlations and scaling trend.
The correlation strengthens from $n=4$ to $n=12$, approaching deterministic behavior for $C_K^{\max}$ and $C_{\mathrm{sat}}$.
The $n=7$ anomaly is discussed in the text.}
\label{tab:scaling}
\centering
\small
\begin{tabular}{@{}ccrrr@{}}
\toprule
$n$ & $N_{\mathrm{codes}}$ & $r(d, \alpha)$ & $r(d, C_K^{\max})$ & $r(d, C_{\mathrm{sat}})$ \\
\midrule
4 & 31 & $-0.58^{***}$ & $-0.50^{**}$ & $-0.44^{*}$ \\
5 & 32 & $-0.62^{***}$ & $-0.62^{***}$ & $-0.54^{**}$ \\
6 & 61 & $-0.72^{***}$ & $-0.49^{***}$ & $-0.40^{**}$ \\
7 & 112 & $-0.21^{*}$ & $-0.29^{**}$ & $-0.22^{*}$ \\
8 & 61 & $-0.87^{***}$ & $-0.52^{***}$ & $-0.53^{***}$ \\
9 & 91 & $-0.84^{***}$ & $-0.58^{***}$ & $-0.56^{***}$ \\
10 & 91 & $-0.87^{***}$ & $-0.87^{***}$ & $-0.87^{***}$ \\
11 & 91 & $-0.88^{***}$ & $-0.85^{***}$ & $-0.86^{***}$ \\
12 & 91 & $-0.91^{***}$ & $-0.99^{***}$ & $-0.993^{***}$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Multi-seed averaging.}
For each code, we compute 10 Krylov trajectories using independent Haar-random initial states $|\phi_s\rangle$ ($s = 1, \ldots, 10$) and measure:
(i)~the \emph{coefficient of variation} (CV) of the trajectory across seeds, quantifying state-sampling noise;
(ii)~the \emph{variance decomposition} into between-code (signal) and within-code (noise) components.
The mean trajectory CV across all 661 codes is $0.034$ (median $0.021$), confirming that code-space projection strongly constrains the dynamics regardless of the specific random state.
The between-code variance accounts for $99.8\%$ of total variance ($236.1$ vs.\ $0.43$ within-code), demonstrating that code structure is overwhelmingly the dominant factor.
Seed-averaged partial correlations strengthen significantly: distance vs.\ $\overline{\alpha}$ reaches $r_{\mathrm{partial}} = -0.70$ ($p = 2.9 \times 10^{-98}$), compared to $-0.52$ for single-seed, confirming that averaging over random projections sharpens the geometry--dynamics signal.

\section{Phase Transition Analysis}
\label{app:phase}

We scanned the anisotropy parameter $\Delta \in [0, 3]$ in the XXZ model to test for phase-transition signatures in Krylov complexity features.
The growth exponent $\alpha$ shows a smooth crossover near $\Delta \approx 1$ (the isotropic Heisenberg point), consistent with the known quantum phase transition from the XY phase ($\Delta < 1$) to the N\'{e}el phase ($\Delta > 1$).
However, due to moderate system sizes ($n \leq 12$), finite-size effects still play a role, though the signal is clearer than at smaller sizes.

\end{document}
